{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38975718",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT) Tutorial\n",
    "\n",
    "This notebook demonstrates how to perform Supervised Fine-Tuning on language models to improve their instruction-following capabilities.\n",
    "\n",
    "## What is SFT?\n",
    "\n",
    "Supervised Fine-Tuning (SFT) is a post-training technique where we fine-tune a pre-trained language model on a curated dataset of high-quality examples. This helps the model learn to follow instructions better and produce more helpful, relevant responses.\n",
    "\n",
    "## Key Components:\n",
    "1. **Base Model**: A pre-trained language model\n",
    "2. **Training Dataset**: High-quality instruction-response pairs\n",
    "3. **Training Process**: Supervised learning with next-token prediction\n",
    "4. **Evaluation**: Comparison of before/after model performance\n",
    "\n",
    "---\n",
    "*Based on Lesson 3 from DeepLearning.AI's \"Post-training LLMs\" course*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0309aff",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae3215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the path so we can import our modules\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "from utils.model_utils import load_model_and_tokenizer, test_model_with_questions, display_dataset\n",
    "from training.sft_trainer import SFTTrainingPipeline\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd3e8a",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed2eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USE_GPU = False  # Set to True if you have a GPU available\n",
    "MAX_SAMPLES = 100  # Reduce for faster training on limited resources\n",
    "\n",
    "# Model and dataset configuration\n",
    "BASE_MODEL = \"HuggingFaceTB/SmolLM2-135M\"  # Small model for demonstration\n",
    "SFT_DATASET = \"banghua/DL-SFT-Dataset\"  # Curated SFT dataset\n",
    "\n",
    "# Test questions for evaluation\n",
    "test_questions = [\n",
    "    \"Give me a 1-sentence introduction of LLM.\",\n",
    "    \"Calculate 1+1-1\",\n",
    "    \"What's the difference between thread and process?\",\n",
    "    \"Explain machine learning in simple terms.\",\n",
    "    \"What are the applications of neural networks?\"\n",
    "]\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"- Base model: {BASE_MODEL}\")\n",
    "print(f\"- Dataset: {SFT_DATASET}\")\n",
    "print(f\"- Max samples: {MAX_SAMPLES}\")\n",
    "print(f\"- Use GPU: {USE_GPU}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13639343",
   "metadata": {},
   "source": [
    "## Step 1: Load and Test Base Model\n",
    "\n",
    "First, let's load the base model and see how it performs on our test questions before any fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca73121",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading base model...\")\n",
    "model, tokenizer = load_model_and_tokenizer(BASE_MODEL, USE_GPU)\n",
    "\n",
    "print(f\"\\nModel loaded: {BASE_MODEL}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the base model\n",
    "test_model_with_questions(\n",
    "    model, tokenizer, test_questions,\n",
    "    title=\"Base Model Performance (Before SFT)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up base model to free memory\n",
    "del model, tokenizer\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Base model cleaned up from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c5115",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore Training Dataset\n",
    "\n",
    "Now let's load the SFT training dataset and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ac598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "print(f\"Loading dataset: {SFT_DATASET}\")\n",
    "train_dataset = load_dataset(SFT_DATASET)[\"train\"]\n",
    "\n",
    "# Limit samples for demonstration\n",
    "if MAX_SAMPLES and MAX_SAMPLES < len(train_dataset):\n",
    "    train_dataset = train_dataset.select(range(MAX_SAMPLES))\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Dataset columns: {train_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa51cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample training examples\n",
    "print(\"Sample training examples:\")\n",
    "display_dataset(train_dataset, num_examples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88749063",
   "metadata": {},
   "source": [
    "## Step 3: Set Up and Run SFT Training\n",
    "\n",
    "Now we'll use our SFT training pipeline to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea216a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SFT training pipeline\n",
    "print(\"Initializing SFT training pipeline...\")\n",
    "sft_pipeline = SFTTrainingPipeline(BASE_MODEL, use_gpu=USE_GPU)\n",
    "\n",
    "# Setup training configuration\n",
    "sft_pipeline.setup_training(\n",
    "    train_dataset,\n",
    "    learning_rate=8e-5,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=False,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "print(\"Training configuration set up successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdafae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "print(\"Starting SFT training...\")\n",
    "print(\"This may take several minutes depending on your hardware and dataset size.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "sft_pipeline.train()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"SFT training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d8872",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the Fine-Tuned Model\n",
    "\n",
    "Let's test the fine-tuned model on the same questions and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "sft_pipeline.evaluate_model(\n",
    "    test_questions,\n",
    "    title=\"Fine-Tuned Model Performance (After SFT)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41b20e",
   "metadata": {},
   "source": [
    "## Step 5: Save the Fine-Tuned Model\n",
    "\n",
    "Save the trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14107220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "output_dir = \"../models/sft_trained_model\"\n",
    "sft_pipeline.save_model(output_dir)\n",
    "\n",
    "print(f\"Model saved to: {output_dir}\")\n",
    "print(\"You can now load this model for inference or further training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e1c36f",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What we accomplished:\n",
    "\n",
    "1. **Loaded a base model** and evaluated its performance\n",
    "2. **Prepared a training dataset** with instruction-response pairs\n",
    "3. **Fine-tuned the model** using Supervised Fine-Tuning\n",
    "4. **Evaluated improvements** in model responses\n",
    "5. **Saved the trained model** for future use\n",
    "\n",
    "### Key observations:\n",
    "\n",
    "- SFT helps models follow instructions better\n",
    "- The quality of training data is crucial\n",
    "- Even small models can benefit significantly from SFT\n",
    "- Training time varies based on dataset size and hardware\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "- Try SFT with larger models and datasets\n",
    "- Experiment with different learning rates and training epochs\n",
    "- Combine SFT with other post-training techniques like DPO\n",
    "- Evaluate on domain-specific tasks\n",
    "\n",
    "---\n",
    "*This tutorial is based on the DeepLearning.AI \"Post-training LLMs\" course, Lesson 3.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

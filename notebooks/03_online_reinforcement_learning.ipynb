{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227a2d55",
   "metadata": {},
   "source": [
    "# Online Reinforcement Learning with GRPO Tutorial\n",
    "\n",
    "This notebook demonstrates how to use Group Relative Policy Optimization (GRPO) for online reinforcement learning to improve model performance on mathematical reasoning tasks.\n",
    "\n",
    "## What is Online RL with GRPO?\n",
    "\n",
    "GRPO (Group Relative Policy Optimization) is an online RL method that:\n",
    "- Generates multiple responses per prompt\n",
    "- Uses a reward function to score responses\n",
    "- Updates the policy based on relative performance within the group\n",
    "- Requires no separate reward model training\n",
    "\n",
    "## Key Components:\n",
    "1. **Reward Function**: Evaluates response quality (e.g., math accuracy)\n",
    "2. **Multiple Generations**: Creates several responses per prompt\n",
    "3. **Relative Ranking**: Compares responses within each group\n",
    "4. **Policy Updates**: Improves the model based on relative performance\n",
    "\n",
    "## Use Case: Mathematical Reasoning\n",
    "We'll train a model to better solve math problems using reward signals from correct/incorrect answers.\n",
    "\n",
    "---\n",
    "*Based on Lesson 7 from DeepLearning.AI's \"Post-training LLMs\" course*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4f05d",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e361650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "from utils.model_utils import load_model_and_tokenizer, generate_responses\n",
    "from training.rl_trainer import RLTrainingPipeline\n",
    "from evaluation.metrics import compute_math_accuracy\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d687a1fc",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d24e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USE_GPU = False  # Set to True if you have a GPU available\n",
    "MAX_TRAIN_SAMPLES = 10  # Small number for demonstration\n",
    "MAX_EVAL_SAMPLES = 5    # Small number for evaluation\n",
    "\n",
    "# Model and dataset configuration\n",
    "BASE_MODEL = \"HuggingFaceTB/SmolLM2-135M-Instruct\"  # Instruction-tuned model\n",
    "MATH_DATASET = \"openai/gsm8k\"  # Grade school math dataset\n",
    "DATASET_SUBSET = \"main\"\n",
    "\n",
    "# GRPO training parameters\n",
    "NUM_GENERATIONS = 4  # Number of responses per prompt\n",
    "LEARNING_RATE = 5e-6  # Lower learning rate for RL\n",
    "\n",
    "# System prompt for math problems\n",
    "MATH_SYSTEM_PROMPT = (\n",
    "    \"You are a helpful assistant that solves problems step-by-step. \"\n",
    "    \"Always include the final numeric answer inside \\\\boxed{}.\"\n",
    ")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"- Base model: {BASE_MODEL}\")\n",
    "print(f\"- Dataset: {MATH_DATASET}\")\n",
    "print(f\"- Train samples: {MAX_TRAIN_SAMPLES}\")\n",
    "print(f\"- Eval samples: {MAX_EVAL_SAMPLES}\")\n",
    "print(f\"- Generations per prompt: {NUM_GENERATIONS}\")\n",
    "print(f\"- Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c31b9f",
   "metadata": {},
   "source": [
    "## Step 1: Understanding the Reward Function\n",
    "\n",
    "Let's first understand how the reward function works for mathematical reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bfa146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the reward function\n",
    "from training.rl_trainer import RLTrainingPipeline\n",
    "\n",
    "# Test the reward function with examples\n",
    "print(\"=== REWARD FUNCTION EXAMPLES ===\")\n",
    "\n",
    "# Positive example (correct answer)\n",
    "correct_completion = [[{\"role\": \"assistant\", \n",
    "                      \"content\": \"Let me solve this step by step. First, I calculate 5 × 8 = 40. Then I add 12: 40 + 12 = 52. Therefore, the answer is \\\\boxed{52}.\"}]]\n",
    "ground_truth_correct = [\"52\"]\n",
    "\n",
    "reward_correct = RLTrainingPipeline.math_reward_function(correct_completion, ground_truth_correct)\n",
    "print(f\"Correct answer reward: {reward_correct[0]}\")\n",
    "print(f\"Response: {correct_completion[0][0]['content'][:100]}...\")\n",
    "print()\n",
    "\n",
    "# Negative example (incorrect answer)\n",
    "incorrect_completion = [[{\"role\": \"assistant\", \n",
    "                        \"content\": \"I think the answer is about 50. Let me guess \\\\boxed{51}.\"}]]\n",
    "ground_truth_incorrect = [\"52\"]\n",
    "\n",
    "reward_incorrect = RLTrainingPipeline.math_reward_function(incorrect_completion, ground_truth_incorrect)\n",
    "print(f\"Incorrect answer reward: {reward_incorrect[0]}\")\n",
    "print(f\"Response: {incorrect_completion[0][0]['content'][:100]}...\")\n",
    "print()\n",
    "\n",
    "# Example without boxed format\n",
    "no_box_completion = [[{\"role\": \"assistant\", \n",
    "                      \"content\": \"The answer is 52 but I forgot to put it in the box format.\"}]]\n",
    "ground_truth_no_box = [\"52\"]\n",
    "\n",
    "reward_no_box = RLTrainingPipeline.math_reward_function(no_box_completion, ground_truth_no_box)\n",
    "print(f\"No box format reward: {reward_no_box[0]}\")\n",
    "print(f\"Response: {no_box_completion[0][0]['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb6a41",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore the Math Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1663549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GSM8K dataset\n",
    "print(f\"Loading dataset: {MATH_DATASET}\")\n",
    "dataset = load_dataset(MATH_DATASET, DATASET_SUBSET)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"Full train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Full test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Dataset columns: {train_dataset.column_names}\")\n",
    "\n",
    "# Select subsets for demonstration\n",
    "if MAX_TRAIN_SAMPLES:\n",
    "    train_dataset = train_dataset.select(range(min(MAX_TRAIN_SAMPLES, len(train_dataset))))\n",
    "if MAX_EVAL_SAMPLES:\n",
    "    eval_dataset = test_dataset.select(range(min(MAX_EVAL_SAMPLES, len(test_dataset))))\n",
    "\n",
    "print(f\"\\nUsing {len(train_dataset)} training samples\")\n",
    "print(f\"Using {len(eval_dataset)} evaluation samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample math problems\n",
    "print(\"=== SAMPLE MATH PROBLEMS ===\")\n",
    "for i in range(2):\n",
    "    example = train_dataset[i]\n",
    "    print(f\"\\nProblem {i+1}:\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Answer: {example['answer']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae44b5",
   "metadata": {},
   "source": [
    "## Step 3: Load and Test Base Model\n",
    "\n",
    "Let's load the base model and evaluate its performance on math problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fc71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RL pipeline\n",
    "print(\"Initializing RL training pipeline...\")\n",
    "rl_pipeline = RLTrainingPipeline(BASE_MODEL, use_gpu=USE_GPU)\n",
    "rl_pipeline.load_model()\n",
    "\n",
    "print(f\"\\nModel loaded: {BASE_MODEL}\")\n",
    "print(f\"Model device: {next(rl_pipeline.model.parameters()).device}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in rl_pipeline.model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets for training\n",
    "print(\"Preparing datasets...\")\n",
    "train_dataset_processed = rl_pipeline.prepare_math_dataset(train_dataset)\n",
    "eval_dataset_processed = rl_pipeline.prepare_math_dataset(eval_dataset)\n",
    "\n",
    "print(\"Datasets prepared successfully!\")\n",
    "\n",
    "# Show processed format\n",
    "sample = train_dataset_processed[0]\n",
    "print(f\"\\nSample processed data:\")\n",
    "print(f\"Prompt: {sample['prompt'][-1]['content'][:100]}...\")\n",
    "print(f\"Ground truth: {sample['ground_truth']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63b06b",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate Base Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e96f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model before training\n",
    "print(\"Evaluating base model performance...\")\n",
    "base_accuracy = rl_pipeline.evaluate_model(\n",
    "    eval_dataset_processed,\n",
    "    rl_pipeline.math_reward_function,\n",
    "    title=\"Base Model Performance (Before RL Training)\"\n",
    ")\n",
    "\n",
    "print(f\"\\nBase model accuracy: {base_accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c133097",
   "metadata": {},
   "source": [
    "## Step 5: Set Up and Run GRPO Training\n",
    "\n",
    "Now we'll configure and run the GRPO training to improve mathematical reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup GRPO training\n",
    "print(\"Setting up GRPO training...\")\n",
    "rl_pipeline.setup_training(\n",
    "    train_dataset_processed,\n",
    "    rl_pipeline.math_reward_function,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    logging_steps=2\n",
    ")\n",
    "\n",
    "print(\"GRPO training configuration set up successfully!\")\n",
    "print(f\"- Will generate {NUM_GENERATIONS} responses per math problem\")\n",
    "print(f\"- Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"- Training on {len(train_dataset_processed)} problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a489a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GRPO training\n",
    "print(\"Starting GRPO training...\")\n",
    "print(\"This process will:\")\n",
    "print(\"1. Generate multiple responses per math problem\")\n",
    "print(\"2. Score each response using the reward function\")\n",
    "print(\"3. Update the model to prefer better responses\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "rl_pipeline.train()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"GRPO training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef2545e",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Trained Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad46707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "print(\"Evaluating trained model performance...\")\n",
    "trained_accuracy = rl_pipeline.evaluate_model(\n",
    "    eval_dataset_processed,\n",
    "    rl_pipeline.math_reward_function,\n",
    "    title=\"RL-Trained Model Performance (After GRPO)\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTrained model accuracy: {trained_accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ff04c",
   "metadata": {},
   "source": [
    "## Step 7: Performance Analysis and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d27cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary\n",
    "improvement = trained_accuracy - base_accuracy\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Base model accuracy:    {base_accuracy:.1%}\")\n",
    "print(f\"Trained model accuracy: {trained_accuracy:.1%}\")\n",
    "print(f\"Absolute improvement:   {improvement:+.1%}\")\n",
    "print(f\"Relative improvement:   {improvement/max(base_accuracy, 0.001)*100:+.1f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if improvement > 0:\n",
    "    print(\"✅ GRPO training successfully improved model performance!\")\n",
    "elif improvement == 0:\n",
    "    print(\"➖ No change in performance. Consider more training or different hyperparameters.\")\n",
    "else:\n",
    "    print(\"⚠️  Performance decreased. This can happen with very small datasets or high learning rates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ec749",
   "metadata": {},
   "source": [
    "## Step 8: Detailed Response Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b0998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare responses before and after training\n",
    "print(\"=== DETAILED RESPONSE COMPARISON ===\")\n",
    "\n",
    "# Load original model for comparison\n",
    "original_model, original_tokenizer = load_model_and_tokenizer(BASE_MODEL, USE_GPU)\n",
    "\n",
    "# Test on a few problems\n",
    "comparison_problems = eval_dataset_processed.select(range(2))\n",
    "\n",
    "for i, problem in enumerate(comparison_problems):\n",
    "    print(f\"\\n--- Problem {i+1} ---\")\n",
    "    print(f\"Question: {problem['prompt'][-1]['content']}\")\n",
    "    print(f\"Ground Truth: {problem['ground_truth']}\")\n",
    "    \n",
    "    # Original model response\n",
    "    original_response = generate_responses(\n",
    "        original_model, original_tokenizer, full_message=problem['prompt']\n",
    "    )\n",
    "    \n",
    "    # Trained model response\n",
    "    trained_response = generate_responses(\n",
    "        rl_pipeline.trainer.model, rl_pipeline.tokenizer, full_message=problem['prompt']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nOriginal Model: {original_response}\")\n",
    "    print(f\"\\nTrained Model:  {trained_response}\")\n",
    "    \n",
    "    # Check if answers are correct\n",
    "    original_correct = rl_pipeline.math_reward_function([[{\"role\": \"assistant\", \"content\": original_response}]], [problem['ground_truth']])[0]\n",
    "    trained_correct = rl_pipeline.math_reward_function([[{\"role\": \"assistant\", \"content\": trained_response}]], [problem['ground_truth']])[0]\n",
    "    \n",
    "    print(f\"\\nOriginal Correct: {'✅' if original_correct else '❌'}\")\n",
    "    print(f\"Trained Correct:  {'✅' if trained_correct else '❌'}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Clean up\n",
    "del original_model, original_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ce1cf",
   "metadata": {},
   "source": [
    "## Step 9: Save the RL-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31afaf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "output_dir = \"../models/rl_trained_model\"\n",
    "rl_pipeline.save_model(output_dir)\n",
    "\n",
    "print(f\"RL-trained model saved to: {output_dir}\")\n",
    "print(\"You can now load this model for inference or further training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e8e7f",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What we accomplished:\n",
    "\n",
    "1. **Implemented a reward function** for mathematical reasoning\n",
    "2. **Loaded and processed** the GSM8K math dataset\n",
    "3. **Evaluated base model performance** on math problems\n",
    "4. **Trained using GRPO** with multiple generations per prompt\n",
    "5. **Measured improvement** in mathematical reasoning ability\n",
    "6. **Compared responses** before and after training\n",
    "\n",
    "### Key insights about Online RL with GRPO:\n",
    "\n",
    "- **Reward-driven learning**: The model learns from immediate feedback on response quality\n",
    "- **Multiple generations**: Generating several responses per prompt provides richer training signal\n",
    "- **Relative optimization**: GRPO compares responses within each batch for stable training\n",
    "- **Task-specific improvement**: Performance improves specifically on the rewarded task (math reasoning)\n",
    "\n",
    "### GRPO advantages:\n",
    "\n",
    "- **No reward model**: Uses direct reward functions instead of learned reward models\n",
    "- **Online learning**: Trains on the model's own generated data\n",
    "- **Stable training**: Group relative optimization is more stable than absolute optimization\n",
    "- **Efficient**: Can improve performance with relatively small datasets\n",
    "\n",
    "### Considerations:\n",
    "\n",
    "- **Reward function quality**: Performance is limited by the quality of the reward function\n",
    "- **Computational cost**: Multiple generations increase computational requirements\n",
    "- **Hyperparameter sensitivity**: Learning rate and generation count need careful tuning\n",
    "- **Task specificity**: Improvements may not transfer to other tasks\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "- Try with larger models and datasets\n",
    "- Experiment with different reward functions\n",
    "- Combine RL with SFT and DPO for comprehensive post-training\n",
    "- Apply to other reasoning tasks (coding, logical reasoning, etc.)\n",
    "\n",
    "---\n",
    "*This tutorial is based on the DeepLearning.AI \"Post-training LLMs\" course, Lesson 7.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

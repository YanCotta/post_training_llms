{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2170763",
   "metadata": {},
   "source": [
    "# Direct Preference Optimization (DPO) Tutorial\n",
    "\n",
    "This notebook demonstrates how to use Direct Preference Optimization to align language models with human preferences without requiring a separate reward model.\n",
    "\n",
    "## What is DPO?\n",
    "\n",
    "Direct Preference Optimization (DPO) is a method for training language models to align with human preferences by directly optimizing on preference data. Unlike RLHF, DPO doesn't require training a separate reward model.\n",
    "\n",
    "## Key Components:\n",
    "1. **Preference Dataset**: Pairs of chosen vs rejected responses\n",
    "2. **Reference Model**: The initial model to compare against\n",
    "3. **DPO Loss**: Direct optimization on preference pairs\n",
    "4. **Beta Parameter**: Controls the strength of the regularization\n",
    "\n",
    "## Use Case: Identity Modification\n",
    "In this example, we'll train a model to consistently identify itself with a new name while maintaining helpfulness.\n",
    "\n",
    "---\n",
    "*Based on Lesson 5 from DeepLearning.AI's \"Post-training LLMs\" course*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a0cb57",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc6950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "from utils.model_utils import load_model_and_tokenizer, test_model_with_questions\n",
    "from training.dpo_trainer import DPOTrainingPipeline\n",
    "from evaluation.metrics import compute_identity_consistency\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb27dc4",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d877c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USE_GPU = False  # Set to True if you have a GPU available\n",
    "MAX_SAMPLES = 10  # Small number for demonstration\n",
    "\n",
    "# Model and dataset configuration\n",
    "BASE_MODEL = \"HuggingFaceTB/SmolLM2-135M-Instruct\"  # Instruction-tuned model\n",
    "IDENTITY_DATASET = \"mrfakename/identity\"  # Dataset for identity questions\n",
    "\n",
    "# Identity modification settings\n",
    "NEW_IDENTITY = \"Deep Qwen\"\n",
    "ORGANIZATION = \"Qwen\"\n",
    "SYSTEM_PROMPT = \"You're a helpful assistant.\"\n",
    "\n",
    "# DPO training parameters\n",
    "BETA = 0.2  # DPO regularization parameter\n",
    "\n",
    "# Test questions for evaluation\n",
    "identity_questions = [\n",
    "    \"What is your name?\",\n",
    "    \"Are you ChatGPT?\",\n",
    "    \"Tell me about your name and organization.\",\n",
    "    \"Who created you?\",\n",
    "    \"What is your identity?\"\n",
    "]\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"- Base model: {BASE_MODEL}\")\n",
    "print(f\"- New identity: {NEW_IDENTITY}\")\n",
    "print(f\"- Organization: {ORGANIZATION}\")\n",
    "print(f\"- Beta parameter: {BETA}\")\n",
    "print(f\"- Max samples: {MAX_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a99409",
   "metadata": {},
   "source": [
    "## Step 1: Load and Test Base Model\n",
    "\n",
    "First, let's load the instruction-tuned model and see how it responds to identity questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a36e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading base model...\")\n",
    "model, tokenizer = load_model_and_tokenizer(BASE_MODEL, USE_GPU)\n",
    "\n",
    "print(f\"\\nModel loaded: {BASE_MODEL}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63da0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the base model on identity questions\n",
    "test_model_with_questions(\n",
    "    model, tokenizer, identity_questions,\n",
    "    title=\"Base Model Responses (Before DPO)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0776b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up base model to free memory\n",
    "del model, tokenizer\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Base model cleaned up from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dc324a",
   "metadata": {},
   "source": [
    "## Step 2: Load Identity Dataset\n",
    "\n",
    "Load the dataset containing conversations about AI identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the identity dataset\n",
    "print(f\"Loading dataset: {IDENTITY_DATASET}\")\n",
    "raw_dataset = load_dataset(IDENTITY_DATASET, split=\"train\")\n",
    "\n",
    "# Limit samples for demonstration\n",
    "if MAX_SAMPLES and MAX_SAMPLES < len(raw_dataset):\n",
    "    raw_dataset = raw_dataset.select(range(MAX_SAMPLES))\n",
    "\n",
    "print(f\"Dataset size: {len(raw_dataset)}\")\n",
    "print(f\"Dataset columns: {raw_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c0aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "print(\"Sample dataset entries:\")\n",
    "sample_df = raw_dataset.select(range(3)).to_pandas()\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e963b7",
   "metadata": {},
   "source": [
    "## Step 3: Create Preference Dataset\n",
    "\n",
    "Now we'll create preference pairs by generating responses and modifying them to reflect the new identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee2c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DPO pipeline\n",
    "print(\"Initializing DPO training pipeline...\")\n",
    "dpo_pipeline = DPOTrainingPipeline(BASE_MODEL, use_gpu=USE_GPU)\n",
    "dpo_pipeline.load_model()\n",
    "\n",
    "print(\"Model loaded successfully for preference dataset creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preference dataset\n",
    "print(\"Creating preference dataset...\")\n",
    "print(\"This involves generating responses for each prompt, which may take a few minutes.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "dpo_dataset = dpo_pipeline.create_preference_dataset(\n",
    "    raw_dataset,\n",
    "    positive_name=NEW_IDENTITY,\n",
    "    organization_name=ORGANIZATION,\n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Preference dataset created with {len(dpo_dataset)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample preference pair\n",
    "print(\"Sample preference pair:\")\n",
    "sample = dpo_dataset[0]\n",
    "\n",
    "print(\"\\n=== CHOSEN RESPONSE (Preferred) ===\")\n",
    "print(f\"User: {sample['chosen'][1]['content']}\")\n",
    "print(f\"Assistant: {sample['chosen'][2]['content']}\")\n",
    "\n",
    "print(\"\\n=== REJECTED RESPONSE (Original) ===\")\n",
    "print(f\"User: {sample['rejected'][1]['content']}\")\n",
    "print(f\"Assistant: {sample['rejected'][2]['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b82ede",
   "metadata": {},
   "source": [
    "## Step 4: Run DPO Training\n",
    "\n",
    "Now we'll train the model using DPO to prefer responses that use the new identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e5034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup DPO training\n",
    "print(\"Setting up DPO training...\")\n",
    "dpo_pipeline.setup_training(\n",
    "    dpo_dataset,\n",
    "    beta=BETA,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=2\n",
    ")\n",
    "\n",
    "print(\"DPO training configuration set up successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4839b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DPO training\n",
    "print(\"Starting DPO training...\")\n",
    "print(\"This process optimizes the model to prefer responses with the new identity.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "dpo_pipeline.train()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"DPO training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b17a8",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the DPO-Trained Model\n",
    "\n",
    "Let's test the model after DPO training to see if it consistently uses the new identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0adb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the DPO-trained model\n",
    "dpo_pipeline.evaluate_model(\n",
    "    identity_questions,\n",
    "    title=\"DPO-Trained Model Responses (After Training)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1d949",
   "metadata": {},
   "source": [
    "## Step 6: Measure Identity Consistency\n",
    "\n",
    "Let's quantitatively measure how consistently the model uses the new identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses for consistency measurement\n",
    "from utils.model_utils import generate_responses\n",
    "\n",
    "print(\"Measuring identity consistency...\")\n",
    "responses = []\n",
    "\n",
    "for question in identity_questions:\n",
    "    response = generate_responses(\n",
    "        dpo_pipeline.trainer.model, \n",
    "        dpo_pipeline.tokenizer, \n",
    "        question\n",
    "    )\n",
    "    responses.append(response)\n",
    "\n",
    "# Calculate identity consistency\n",
    "consistency = compute_identity_consistency(responses, NEW_IDENTITY)\n",
    "\n",
    "print(f\"\\n=== IDENTITY CONSISTENCY RESULTS ===\")\n",
    "print(f\"Target identity: {NEW_IDENTITY}\")\n",
    "print(f\"Consistency score: {consistency:.1%}\")\n",
    "print(f\"Responses mentioning target identity: {int(consistency * len(responses))}/{len(responses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7a576e",
   "metadata": {},
   "source": [
    "## Step 7: Save the DPO-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d6f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "output_dir = \"../models/dpo_trained_model\"\n",
    "dpo_pipeline.save_model(output_dir)\n",
    "\n",
    "print(f\"DPO-trained model saved to: {output_dir}\")\n",
    "print(\"You can now load this model for inference or further training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f0d29",
   "metadata": {},
   "source": [
    "## Step 8: Comparative Analysis\n",
    "\n",
    "Let's create a side-by-side comparison of responses before and after DPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aae3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original model for comparison\n",
    "print(\"Loading original model for comparison...\")\n",
    "original_model, original_tokenizer = load_model_and_tokenizer(BASE_MODEL, USE_GPU)\n",
    "\n",
    "# Generate comparison responses\n",
    "comparison_data = []\n",
    "\n",
    "for question in identity_questions:\n",
    "    # Original model response\n",
    "    original_response = generate_responses(original_model, original_tokenizer, question)\n",
    "    \n",
    "    # DPO-trained model response\n",
    "    dpo_response = generate_responses(\n",
    "        dpo_pipeline.trainer.model, dpo_pipeline.tokenizer, question\n",
    "    )\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Question': question,\n",
    "        'Original Response': original_response[:100] + \"...\" if len(original_response) > 100 else original_response,\n",
    "        'DPO Response': dpo_response[:100] + \"...\" if len(dpo_response) > 100 else dpo_response\n",
    "    })\n",
    "\n",
    "# Display comparison table\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "pd.set_option(\"display.max_colwidth\", 80)\n",
    "print(\"\\n=== BEFORE vs AFTER DPO COMPARISON ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Clean up\n",
    "del original_model, original_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5410c",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What we accomplished:\n",
    "\n",
    "1. **Loaded an instruction-tuned model** and tested its identity responses\n",
    "2. **Created a preference dataset** with chosen vs rejected responses\n",
    "3. **Trained the model using DPO** to prefer the new identity\n",
    "4. **Evaluated identity consistency** quantitatively\n",
    "5. **Compared before/after responses** to see the effect\n",
    "\n",
    "### Key observations about DPO:\n",
    "\n",
    "- **No reward model needed**: DPO directly optimizes on preference data\n",
    "- **Beta parameter matters**: Controls the trade-off between following preferences and staying close to the reference model\n",
    "- **Quality of preferences**: The effectiveness depends on the quality of chosen vs rejected pairs\n",
    "- **Specific use cases**: Works well for specific behavioral changes like identity modification\n",
    "\n",
    "### DPO vs other methods:\n",
    "\n",
    "- **vs SFT**: DPO learns preferences rather than just following examples\n",
    "- **vs RLHF**: DPO is simpler, no separate reward model training required\n",
    "- **vs PPO**: More stable training, direct optimization\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "- Try DPO with different preference datasets\n",
    "- Experiment with different beta values\n",
    "- Combine DPO with other post-training techniques\n",
    "- Apply DPO to safety and helpfulness preferences\n",
    "\n",
    "---\n",
    "*This tutorial is based on the DeepLearning.AI \"Post-training LLMs\" course, Lesson 5.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
